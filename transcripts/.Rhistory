files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
# simple transformations
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
cleaned <- tm_map(combined, toSpace, "/|@|\\|")
# make lowercase
cleaned <- tm_map(cleaned, content_transformer(tolower))
# removing stop words and a couplt of stage directions
cleaned <- tm_map(cleaned, removeWords, c(stopwords("english"), c("int", "ext", "get", "continu", "around", "can", "t", "like", "make", "away")))
# removing punctuation
cleaned <- tm_map(cleaned, removePunctuation)
# Removing bad characters
cleaned <- tm_map(cleaned, toSpace, "â€˜")
cleaned <- tm_map(cleaned, toSpace, "â€œ")
cleaned <- tm_map(cleaned, toSpace, "â€™")
cleaned <- tm_map(cleaned, toSpace, "â€\u009d")
cleaned <- tm_map(cleaned, toSpace, "ã‰")
cleaned <- tm_map(cleaned, toSpace, "ã©")
cleaned <- tm_map(cleaned, toSpace, "â\200\231")
cleaned <- tm_map(cleaned, toSpace, "â\200¦")
# removing numbers
cleaned <- tm_map(cleaned, removeNumbers)
# trimming whitespace
cleaned <- tm_map(cleaned, stripWhitespace)
# stemming
cleaned <- tm_map(cleaned, stemDocument)
# removing names
cleaned <- tm_map(cleaned, removeWords, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
dtm_total
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
library(tm)
library(tidytext)
library(textdata)
library(ggplot2)
library(quanteda)
library(dplyr)
library(wordcloud2)
library(tidyr)
library(topicmodels)
library(readr)
library(factoextra)
#files_combined <- list.files("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts", all.files = FALSE)
#setwd("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts")
transcript_path = "D:/personal_projects/star_wars/transcripts"
files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
# simple transformations
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
cleaned <- tm_map(combined, toSpace, "/|@|\\|")
# make lowercase
cleaned <- tm_map(cleaned, content_transformer(tolower))
# removing stop words and a couplt of stage directions
cleaned <- tm_map(cleaned, removeWords, c(stopwords("english"), c("int", "ext", "get", "continu", "around", "can", "t", "like", "make", "away")))
# removing punctuation
cleaned <- tm_map(cleaned, removePunctuation)
# Removing bad characters
cleaned <- tm_map(cleaned, toSpace, "â€˜")
cleaned <- tm_map(cleaned, toSpace, "â€œ")
cleaned <- tm_map(cleaned, toSpace, "â€™")
cleaned <- tm_map(cleaned, toSpace, "â€\u009d")
cleaned <- tm_map(cleaned, toSpace, "ã‰")
cleaned <- tm_map(cleaned, toSpace, "ã©")
cleaned <- tm_map(cleaned, toSpace, "â\200\231")
cleaned <- tm_map(cleaned, toSpace, "â\200¦")
# removing numbers
cleaned <- tm_map(cleaned, removeNumbers)
# trimming whitespace
cleaned <- tm_map(cleaned, stripWhitespace)
# stemming
cleaned <- tm_map(cleaned, stemDocument)
# removing names
cleaned <- tm_map(cleaned, removeWords, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
source('D:/personal_projects/star_wars/cleaning.R', echo=TRUE)
source('D:/personal_projects/star_wars/cleaning.R', echo=TRUE)
library(tm)
library(tidytext)
library(textdata)
library(ggplot2)
library(quanteda)
library(dplyr)
library(wordcloud2)
library(tidyr)
library(topicmodels)
library(readr)
library(factoextra)
#files_combined <- list.files("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts", all.files = FALSE)
#setwd("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts")
transcript_path = "D:/personal_projects/star_wars/transcripts"
files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
# simple transformations
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
cleaned <- tm_map(combined, toSpace, "/|@|\\|")
# make lowercase
cleaned <- tm_map(cleaned, content_transformer(tolower))
# removing stop words and a couplt of stage directions
cleaned <- tm_map(cleaned, removeWords, c(stopwords("english"), c("int", "ext", "get", "continu", "around", "can", "t", "like", "make", "away")))
# removing punctuation
cleaned <- tm_map(cleaned, removePunctuation)
# Removing bad characters
cleaned <- tm_map(cleaned, toSpace, "â€˜")
cleaned <- tm_map(cleaned, toSpace, "â€œ")
cleaned <- tm_map(cleaned, toSpace, "â€™")
cleaned <- tm_map(cleaned, toSpace, "â€\u009d")
cleaned <- tm_map(cleaned, toSpace, "ã‰")
cleaned <- tm_map(cleaned, toSpace, "ã©")
cleaned <- tm_map(cleaned, toSpace, "â\200\231")
cleaned <- tm_map(cleaned, toSpace, "â\200¦")
# removing numbers
cleaned <- tm_map(cleaned, removeNumbers)
# trimming whitespace
cleaned <- tm_map(cleaned, stripWhitespace)
# stemming
cleaned <- tm_map(cleaned, stemDocument)
# removing names
cleaned <- tm_map(cleaned, removeWords, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
source('D:/personal_projects/star_wars/cleaning.R', echo=TRUE)
cleaned <- tm_map(combined, toSpace, "/|@|\\|")
# make lowercase
cleaned <- tm_map(cleaned, content_transformer(tolower))
# removing stop words and a couplt of stage directions
cleaned <- tm_map(cleaned, removeWords, c(stopwords("english"), c("int", "ext", "get", "continu", "around", "can", "t", "like", "make", "away")))
# removing punctuation
cleaned <- tm_map(cleaned, removePunctuation)
# Removing bad characters
cleaned <- tm_map(cleaned, toSpace, "â€˜")
cleaned <- tm_map(cleaned, toSpace, "â€œ")
cleaned <- tm_map(cleaned, toSpace, "â€™")
cleaned <- tm_map(cleaned, toSpace, "â€\u009d")
cleaned <- tm_map(cleaned, toSpace, "ã‰")
cleaned <- tm_map(cleaned, toSpace, "ã©")
cleaned <- tm_map(cleaned, toSpace, "â\200\231")
cleaned <- tm_map(cleaned, toSpace, "â\200¦")
# removing numbers
cleaned <- tm_map(cleaned, removeNumbers)
# trimming whitespace
cleaned <- tm_map(cleaned, stripWhitespace)
# stemming
cleaned <- tm_map(cleaned, stemDocument)
# removing names
cleaned <- tm_map(cleaned, removeWords, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
dtm_total
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
source('D:/personal_projects/star_wars/cleaning.R')
# removing names
cleaned <- tm_map(cleaned, removeWords, name_list)
library(tm)
library(tidytext)
library(textdata)
library(ggplot2)
library(quanteda)
library(dplyr)
library(wordcloud2)
library(tidyr)
library(topicmodels)
library(readr)
library(factoextra)
library(dplyr)
#files_combined <- list.files("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts", all.files = FALSE)
#setwd("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts")
transcript_path = "D:/personal_projects/star_wars/transcripts"
files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
# simple transformations
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
cleaned <- tm_map(combined, toSpace, "/|@|\\|")
# make lowercase
cleaned <- tm_map(cleaned, content_transformer(tolower))
# removing stop words and a couplt of stage directions
cleaned <- tm_map(cleaned, removeWords, c(stopwords("english"), c("int", "ext", "get", "continu", "around", "can", "t", "like", "make", "away")))
# removing punctuation
cleaned <- tm_map(cleaned, removePunctuation)
# Removing bad characters
cleaned <- tm_map(cleaned, toSpace, "â€˜")
cleaned <- tm_map(cleaned, toSpace, "â€œ")
cleaned <- tm_map(cleaned, toSpace, "â€™")
cleaned <- tm_map(cleaned, toSpace, "â€\u009d")
cleaned <- tm_map(cleaned, toSpace, "ã‰")
cleaned <- tm_map(cleaned, toSpace, "ã©")
cleaned <- tm_map(cleaned, toSpace, "â\200\231")
cleaned <- tm_map(cleaned, toSpace, "â\200¦")
# removing numbers
cleaned <- tm_map(cleaned, removeNumbers)
# trimming whitespace
cleaned <- tm_map(cleaned, stripWhitespace)
# stemming
cleaned <- tm_map(cleaned, stemDocument)
# removing names
cleaned <- tm_map(cleaned, removeWords, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
source('D:/personal_projects/star_wars/functions.R')
source('D:/personal_projects/star_wars/cleaning.R', echo=TRUE)
source('D:/personal_projects/star_wars/cleaning.R')
library(tm)
library(tidytext)
library(textdata)
library(ggplot2)
library(quanteda)
library(dplyr)
library(wordcloud2)
library(tidyr)
library(topicmodels)
library(readr)
library(factoextra)
library(dplyr)
#files_combined <- list.files("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts", all.files = FALSE)
#setwd("D:/Grad 2nd year/Winter Quarter/Text Analysis and Mining/Final/Transcripts")
transcript_path = "D:/personal_projects/star_wars/transcripts"
files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
cleaned <- get_corpus(combined, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
source('D:/personal_projects/star_wars/cleaning.R')
transcript_path = "D:/personal_projects/star_wars/transcripts"
files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
cleaned <- get_corpus(combined, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
get_corpus <- function(combined, name_list){
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
cleaned <- tm_map(combined, toSpace, "/|@|\\|")
# make lowercase
cleaned <- tm_map(cleaned, content_transformer(tolower))
# removing stop words and a couplt of stage directions
cleaned <- tm_map(cleaned, removeWords, c(stopwords("english"), c("int", "ext", "get", "continu", "around", "can", "t", "like", "make", "away")))
# removing punctuation
cleaned <- tm_map(cleaned, removePunctuation)
# Removing bad characters
cleaned <- tm_map(cleaned, toSpace, "â€˜")
cleaned <- tm_map(cleaned, toSpace, "â€œ")
cleaned <- tm_map(cleaned, toSpace, "â€™")
cleaned <- tm_map(cleaned, toSpace, "â€\u009d")
cleaned <- tm_map(cleaned, toSpace, "ã‰")
cleaned <- tm_map(cleaned, toSpace, "ã©")
cleaned <- tm_map(cleaned, toSpace, "â\200\231")
cleaned <- tm_map(cleaned, toSpace, "â\200¦")
# removing numbers
cleaned <- tm_map(cleaned, removeNumbers)
# trimming whitespace
cleaned <- tm_map(cleaned, stripWhitespace)
# stemming
cleaned <- tm_map(cleaned, stemDocument)
# removing names
cleaned <- tm_map(cleaned, removeWords, name_list)
return(cleaned)
}
sentiment_grabber <- function(tidy_frame){
bing_words <- tidy_frame %>% inner_join(get_sentiments("bing"), by = c(term = "word"))
return(aggregate(bing_words$count, by = list(Category = bing_words$sentiment), FUN = sum))
}
transcript_path = "D:/personal_projects/star_wars/transcripts"
files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
cleaned <- get_corpus(combined, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
library(tm)
library(dplyr)
library(tidytext)
library(textdata)
library(ggplot2)
library(quanteda)
library(dplyr)
library(wordcloud2)
library(tidyr)
library(topicmodels)
library(readr)
library(factoextra)
source("D:/personal_projects/star_wars/functions.R")
source("D:/personal_projects/star_wars/cleaning.R")
tstat_lexdiv <- textstat_lexdiv(dfm_total)
plot(tstat_lexdiv$TTR, type = "l", xaxt = "n", xlab = "Film Number", ylab = "Lexical Diversity Score", main = "Lexical Diversity by Movie") + grid() + axis(1, at = seq_len(nrow(tstat_lexdiv)))
source('D:/personal_projects/star_wars/Cluster Analysis.R')
library(tm)
library(dplyr)
library(tidytext)
library(textdata)
library(ggplot2)
library(quanteda)
library(dplyr)
library(wordcloud2)
library(tidyr)
library(topicmodels)
library(readr)
library(factoextra)
source("D:/personal_projects/star_wars/functions.R")
source("D:/personal_projects/star_wars/cleaning.R")
mat <- as.matrix(dtm_total)
distMatrix <- dist(mat, method="euclidian")
groups <- hclust(distMatrix,method="ward.D")
plot(groups, cex=0.9, hang=-1)
rect.hclust(groups, k=3)
clustering.kmeans <- kmeans(distMatrix, 3, nstart = 10)
fviz_cluster(clustering.kmeans, data = mat,
stand = TRUE,
palette = "Dark",
ggtheme = theme_bw())
source('D:/personal_projects/star_wars/Cluster Analysis.R')
library(tm)
library(dplyr)
library(tidytext)
library(textdata)
library(ggplot2)
library(quanteda)
library(dplyr)
library(wordcloud2)
library(tidyr)
library(topicmodels)
library(readr)
library(factoextra)
source("D:/personal_projects/star_wars/functions.R")
source("D:/personal_projects/star_wars/cleaning.R")
t.h <- data.frame(token_total %>% count(word))
wordcloud2(t.h , minSize = 60, backgroundColor = "black", color = "random-light")
tstat_lexdiv <- textstat_lexdiv(dfm_total)
plot(tstat_lexdiv$TTR, type = "l", xaxt = "n", xlab = "Film Number", ylab = "Lexical Diversity Score", main = "Lexical Diversity by Movie") + grid() + axis(1, at = seq_len(nrow(tstat_lexdiv)))
transcript_path = "D:/personal_projects/star_wars/transcripts"
files_combined <- list.files(transcript_path, all.files = FALSE)
setwd(transcript_path)
combined <- Corpus(URISource(files_combined),readerControl = list(reader = readPlain))
# Getting the star wars data ready.
names <- starwars
names <- separate(names, name, into = c("names1", "names2"), sep = " ", fill = "right", remove = TRUE)
# The columns that matter are: names1,names2,homeworld,  species
name_list <- c()
for(columnus in c(names$names1, names$names2, names$homeworld)){
columnus <- tolower(columnus)
# removing punctuation
columnus <- removePunctuation(columnus)
# stemming
columnus <- stemDocument(columnus)
name_list <- append(name_list, columnus)
}
name_list <- append(name_list, c("artoo", "kylo", "chewi", "chancellor", "threepio", "aayla", "padm", "rose", "ren", "ewok", "hux", "huldo", "hoth", "zorii", "jannah", "exegol", "holdo", "babu", "amilyn", "enric", "queen", "wayfind", "snoke", "capt", "grunt", "skiff", "oscil", "armitag", "starkil", "beed", "chirp", "feder", "canadi", "castl", "maz", "obi", "wan", "peavy", "rogu", "kitster", "falcon", "gungan", "peavey", "espa", "ochi", "fode", "contd", "frik", "kijimi","beaumont","eirta","piett", "oli","millennium","snowspeed", "bunker", "sub","wesa","rathtar","rieekan", "talli", "unkar","massassi","tipoca","mos", "threepio", "cpo", "s"))
cleaned <- get_corpus(combined, name_list)
dtm_total <- cleaned %>% DocumentTermMatrix() %>% removeSparseTerms(.85)
#dtm_total <- removeSparseTerms(dtm_total, 0.85)
word_l <- strsplit(unlist(sapply(cleaned, '[', "content")), "[^A-Za-z']+")
tibble_total <- tibble(id = names(word_l), text=unlist(sapply(cleaned, '[', "content")))
token_total <- tibble_total %>% unnest_tokens(word, text)
#Lexical Diversity
total_tidy <- tidy(dtm_total)
dfm_total <- total_tidy %>% cast_dfm(document, term, count)
dfm_total
